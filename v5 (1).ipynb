{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22d856e9-7442-49de-8a1b-02692a95634d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting imports and dataset preparation...\n",
      "Datasets loaded.\n",
      "Starting training process...\n",
      "[Classifier Epoch 1] Loss: 204.2144\n",
      "[Classifier Epoch 2] Loss: 72.7412\n",
      "[Classifier Epoch 3] Loss: 49.0540\n",
      "[Classifier Epoch 4] Loss: 36.0197\n",
      "[Classifier Epoch 5] Loss: 27.8276\n",
      "🎯 Evaluation Accuracy: 0.9776\n",
      "\n",
      "Before Unlearning:\n",
      "Accuracy: 0.9776\n",
      "Graph built with 235146 nodes and 470280 edges.\n",
      "\n",
      "Attempt 1: Trying forget point idx 80 with label 9\n",
      "→ Margin: 0.0797 | Feasible: Yes\n",
      "✅ Forget point is feasible for unlearning.\n",
      "\n",
      "🔧 Starting GCN training to forget data point (label: 9)\n",
      "✅ Mask applied to model weights.\n",
      "[Epoch   1/50] Loss: 3.0776 | CE: 3.0482 | KL: 0.0294\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "[Epoch  10/50] Loss: 2.5271 | CE: 2.4999 | KL: 0.0272\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "[Epoch  20/50] Loss: 2.7109 | CE: 2.6861 | KL: 0.0248\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "[Epoch  30/50] Loss: 2.8793 | CE: 2.8567 | KL: 0.0226\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "[Epoch  40/50] Loss: 2.5714 | CE: 2.5510 | KL: 0.0205\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "[Epoch  50/50] Loss: 2.5872 | CE: 2.5687 | KL: 0.0185\n",
      "✅ GCN training complete in 0.13s\n",
      "\n",
      "✅ Forget point idx 80 accepted for unlearning.\n",
      "\n",
      "Applying mask to classifier weights for unlearning...\n",
      "Confidence before unlearning on forget point: 0.0312\n",
      "✅ Mask applied to model weights.\n",
      "Confidence after unlearning on forget point: 0.1060\n",
      "🎯 Evaluation Accuracy: 0.9767\n",
      "\n",
      "After Unlearning:\n",
      "Accuracy: 0.9767\n"
     ]
    }
   ],
   "source": [
    "# Full OPM Implementation with Confidence Checks Before/After Unlearning\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.batch_size = 128\n",
    "        self.test_batch_size = 1000\n",
    "        self.epochs = 5\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.9\n",
    "        self.mask_lr = 0.001\n",
    "        self.mask_epochs = 50\n",
    "        self.kl_coeff = 1.0\n",
    "        self.temperature = 0.5\n",
    "        self.top_k_ratio = 0.2\n",
    "        self.num_forget = 10\n",
    "        self.prior = 0.5\n",
    "        self.num_params = 0  # filled later\n",
    "        self.edge_index = None\n",
    "        self.overlap_threshold = 0.9  # cosine similarity threshold\n",
    "        self.max_attempts = 50\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Dataset and DataLoader\n",
    "print(\"Starting imports and dataset preparation...\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.test_batch_size, shuffle=False)\n",
    "print(\"Datasets loaded.\")\n",
    "\n",
    "# Model\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# GCN for mask generation\n",
    "class GCNMaskNet(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, 32)\n",
    "        self.conv2 = GCNConv(32, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = torch.sigmoid(self.conv2(x, edge_index))\n",
    "        return x\n",
    "\n",
    "# Helper functions\n",
    "def copy_model(model):\n",
    "    return copy.deepcopy(model)\n",
    "\n",
    "def cosine_similarity(x, y):\n",
    "    return F.cosine_similarity(x.view(1, -1), y.view(1, -1))\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    noise = torch.rand_like(logits)\n",
    "    gumbel = -torch.log(-torch.log(noise + 1e-20) + 1e-20)\n",
    "    return torch.sigmoid((logits + gumbel) / temperature)\n",
    "\n",
    "def kl_divergence(p, prior):\n",
    "    p = torch.clamp(p, 1e-6, 1 - 1e-6)\n",
    "    prior = torch.full_like(p, prior)\n",
    "    return (p * (p / prior).log() + (1 - p) * ((1 - p) / (1 - prior)).log()).mean()\n",
    "\n",
    "def apply_mask_to_model(model, mask, config):\n",
    "    offset = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            numel = param.numel()\n",
    "            param.data *= mask[offset:offset+numel].view(param.shape)\n",
    "            offset += numel\n",
    "    print(\"✅ Mask applied to model weights.\")\n",
    "\n",
    "def train_classifier(model, loader, config):\n",
    "    model.train()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config.lr, momentum=config.momentum)\n",
    "    for epoch in range(config.epochs):\n",
    "        total_loss = 0\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(config.device), target.to(config.device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"[Classifier Epoch {epoch+1}] Loss: {total_loss:.4f}\")\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(config.device), target.to(config.device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    acc = correct / len(loader.dataset)\n",
    "    print(f\"🎯 Evaluation Accuracy: {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "def is_unlearning_feasible_margin(model, x, y, threshold=0.0):\n",
    "    output = model(x.unsqueeze(0))\n",
    "    probs = F.softmax(output, dim=1)\n",
    "    top2 = torch.topk(probs, 2).values.squeeze()\n",
    "    margin = top2[0] - top2[1]\n",
    "    print(f\"→ Margin: {margin.item():.4f} | Feasible: {'Yes' if margin > threshold else 'No'}\")\n",
    "    return margin > threshold\n",
    "\n",
    "def check_mask_overlap(new_mask, previous_masks, threshold=0.9):\n",
    "    if not previous_masks:\n",
    "        return True\n",
    "    for i, prev in enumerate(previous_masks):\n",
    "        sim = cosine_similarity(new_mask, prev).item()\n",
    "        print(f\"→ Cosine similarity with previous mask {i}: {sim:.4f}\")\n",
    "        if sim > threshold:\n",
    "            print(\"⛔ Overlap too high. Rejecting mask.\\n\")\n",
    "            return False\n",
    "    print(\"✅ Mask is unique (overlap below threshold).\\n\")\n",
    "    return True\n",
    "\n",
    "def build_graph_from_model(model):\n",
    "    sizes = [param.numel() for param in model.parameters()]\n",
    "    total_params = sum(sizes)\n",
    "    config.num_params = total_params\n",
    "    x = torch.ones((total_params, 1)).to(config.device)\n",
    "    edge_index = []\n",
    "    idx = 0\n",
    "    for size in sizes:\n",
    "        for i in range(size - 1):\n",
    "            edge_index.append([idx + i, idx + i + 1])\n",
    "            edge_index.append([idx + i + 1, idx + i])\n",
    "        idx += size\n",
    "    edge_index = torch.tensor(edge_index).t().contiguous().to(config.device)\n",
    "    config.edge_index = edge_index\n",
    "    print(f\"Graph built with {total_params} nodes and {edge_index.shape[1]} edges.\")\n",
    "\n",
    "def train_mask_generator(mask_net, classifier, forget_data, config):\n",
    "    mask_net.train()\n",
    "    optimizer = torch.optim.Adam(mask_net.parameters(), lr=config.mask_lr)\n",
    "\n",
    "    x_forget, y_forget = forget_data\n",
    "    x_forget = x_forget.to(config.device).unsqueeze(0)\n",
    "    y_forget = y_forget.to(config.device)\n",
    "\n",
    "    x_init = torch.ones(config.num_params, 1).to(config.device)\n",
    "    edge_index = config.edge_index.to(config.device)\n",
    "\n",
    "    print(f\"\\n🔧 Starting GCN training to forget data point (label: {y_forget.item()})\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(config.mask_epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        mask_logits = mask_net(x_init, edge_index).squeeze()\n",
    "        mask_probs = torch.sigmoid(mask_logits)\n",
    "        mask_sampled = gumbel_softmax_sample(mask_logits, temperature=config.temperature)\n",
    "\n",
    "        temp_model = copy_model(classifier).to(config.device)\n",
    "        apply_mask_to_model(temp_model, mask_sampled, config)\n",
    "\n",
    "        output = temp_model(x_forget)\n",
    "        ce_loss = F.cross_entropy(output, y_forget.unsqueeze(0))\n",
    "        kl = kl_divergence(mask_probs, config.prior)\n",
    "\n",
    "        loss = ce_loss + config.kl_coeff * kl\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"[Epoch {epoch+1:3d}/{config.mask_epochs}] Loss: {loss.item():.4f} | CE: {ce_loss.item():.4f} | KL: {kl.item():.4f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"✅ GCN training complete in {end_time - start_time:.2f}s\\n\")\n",
    "    return mask_logits.detach()\n",
    "\n",
    "def main():\n",
    "    print(\"Starting training process...\")\n",
    "    classifier = SimpleClassifier().to(config.device)\n",
    "    train_classifier(classifier, train_loader, config)\n",
    "\n",
    "    acc_before = evaluate_model(classifier, test_loader)\n",
    "    print(f\"\\nBefore Unlearning:\\nAccuracy: {acc_before:.4f}\")\n",
    "\n",
    "    # Build graph once after training\n",
    "    build_graph_from_model(classifier)\n",
    "\n",
    "    accepted_masks = []\n",
    "    forget_x, forget_y, forget_idx, forget_mask = None, None, None, None\n",
    "\n",
    "    for attempt in range(1, config.max_attempts + 1):\n",
    "        idx = random.randint(0, len(train_dataset) - 1)\n",
    "        x_i, y_i = train_dataset[idx]\n",
    "        x_i = x_i.to(config.device)\n",
    "        y_i = torch.tensor(y_i).to(config.device)\n",
    "        print(f\"\\nAttempt {attempt}: Trying forget point idx {idx} with label {y_i.item()}\")\n",
    "\n",
    "        if not is_unlearning_feasible_margin(classifier, x_i, y_i):\n",
    "            print(\"❌ Not feasible by margin, trying another point...\")\n",
    "            continue\n",
    "\n",
    "        print(\"✅ Forget point is feasible for unlearning.\")\n",
    "\n",
    "        mask_net = GCNMaskNet(1).to(config.device)\n",
    "        mask_logits = train_mask_generator(mask_net, classifier, (x_i, y_i), config)\n",
    "\n",
    "        if not check_mask_overlap(mask_logits, accepted_masks, config.overlap_threshold):\n",
    "            print(\"❌ Mask overlap too high, trying next point...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"✅ Forget point idx {idx} accepted for unlearning.\")\n",
    "        accepted_masks.append(mask_logits)\n",
    "        forget_x, forget_y, forget_idx, forget_mask = x_i, y_i, idx, mask_logits\n",
    "        break\n",
    "    else:\n",
    "        print(\"❌ No feasible forget point found within max attempts passing overlap checks.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nApplying mask to classifier weights for unlearning...\")\n",
    "    # Confidence before applying mask\n",
    "    output_before = classifier(forget_x.unsqueeze(0))\n",
    "    conf_before = F.softmax(output_before, dim=1)[0, forget_y].item()\n",
    "    print(f\"Confidence before unlearning on forget point: {conf_before:.4f}\")\n",
    "\n",
    "    apply_mask_to_model(classifier, forget_mask, config)\n",
    "\n",
    "    # Confidence after applying mask\n",
    "    output_after = classifier(forget_x.unsqueeze(0))\n",
    "    conf_after = F.softmax(output_after, dim=1)[0, forget_y].item()\n",
    "    print(f\"Confidence after unlearning on forget point: {conf_after:.4f}\")\n",
    "\n",
    "    acc_after = evaluate_model(classifier, test_loader)\n",
    "    print(f\"\\nAfter Unlearning:\\nAccuracy: {acc_after:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "487ac960-65fc-449f-8947-ab643c319420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting imports and dataset preparation...\n",
      "Datasets loaded.\n",
      "Starting training process...\n",
      "[Classifier Epoch 1] Loss: 204.2144\n",
      "[Classifier Epoch 2] Loss: 72.7412\n",
      "[Classifier Epoch 3] Loss: 49.0540\n",
      "[Classifier Epoch 4] Loss: 36.0197\n",
      "[Classifier Epoch 5] Loss: 27.8276\n",
      "🎯 Evaluation Accuracy: 0.9776\n",
      "\n",
      "Before Unlearning:\n",
      "Accuracy: 0.9776\n",
      "Graph built with 235146 nodes and 470280 edges.\n",
      "\n",
      "Attempt 1: Trying forget point idx 80 with label 9\n",
      "→ Margin: 0.0797 | Feasible: Yes\n",
      "✅ Forget point is feasible for unlearning.\n",
      "\n",
      "🔧 Starting GCN training to forget data point (label: 9)\n",
      "✅ Mask applied to model weights.\n",
      "[Epoch   1/50] Loss: 3.0776 | CE: 3.0482 | KL: 0.0294\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "[Epoch  10/50] Loss: 2.5271 | CE: 2.4999 | KL: 0.0272\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "[Epoch  20/50] Loss: 2.7109 | CE: 2.6861 | KL: 0.0248\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "[Epoch  30/50] Loss: 2.8793 | CE: 2.8567 | KL: 0.0226\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "[Epoch  40/50] Loss: 2.5714 | CE: 2.5510 | KL: 0.0205\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "✅ Mask applied to model weights.\n",
      "[Epoch  50/50] Loss: 2.5872 | CE: 2.5687 | KL: 0.0185\n",
      "✅ GCN training complete in 0.22s\n",
      "\n",
      "✅ Forget point idx 80 accepted for unlearning.\n",
      "\n",
      "Applying mask to classifier weights for unlearning...\n",
      "\n",
      "📊 Confidence BEFORE unlearning on forget point (label 9):\n",
      "  Class 0: 0.3836\n",
      "  Class 1: 0.0030\n",
      "  Class 2: 0.0110\n",
      "  Class 3: 0.0277\n",
      "  Class 4: 0.0004\n",
      "  Class 5: 0.1966\n",
      "  Class 6: 0.0417\n",
      "  Class 7: 0.0009\n",
      "  Class 8: 0.3039\n",
      "  Class 9: 0.0312\n",
      "→ Target Class 9 Confidence: 0.0312\n",
      "\n",
      "✅ Mask applied to model weights.\n",
      "\n",
      "📊 Confidence AFTER unlearning on forget point (label 9):\n",
      "  Class 0: 0.1112\n",
      "  Class 1: 0.0880\n",
      "  Class 2: 0.0969\n",
      "  Class 3: 0.1031\n",
      "  Class 4: 0.0813\n",
      "  Class 5: 0.1134\n",
      "  Class 6: 0.1020\n",
      "  Class 7: 0.0815\n",
      "  Class 8: 0.1165\n",
      "  Class 9: 0.1060\n",
      "→ Target Class 9 Confidence: 0.1060\n",
      "\n",
      "🎯 Evaluation Accuracy: 0.9767\n",
      "\n",
      "After Unlearning:\n",
      "Accuracy: 0.9767\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.batch_size = 128\n",
    "        self.test_batch_size = 1000\n",
    "        self.epochs = 5\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.9\n",
    "        self.mask_lr = 0.001\n",
    "        self.mask_epochs = 50\n",
    "        self.kl_coeff = 1.0\n",
    "        self.temperature = 0.5\n",
    "        self.top_k_ratio = 0.2\n",
    "        self.num_forget = 10\n",
    "        self.prior = 0.5\n",
    "        self.num_params = 0  # filled later\n",
    "        self.edge_index = None\n",
    "        self.overlap_threshold = 0.9\n",
    "        self.max_attempts = 50\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Dataset and DataLoader\n",
    "print(\"Starting imports and dataset preparation...\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.test_batch_size, shuffle=False)\n",
    "print(\"Datasets loaded.\")\n",
    "\n",
    "# Model definition\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# GCN for mask generation\n",
    "class GCNMaskNet(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, 32)\n",
    "        self.conv2 = GCNConv(32, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = torch.sigmoid(self.conv2(x, edge_index))\n",
    "        return x\n",
    "\n",
    "# Helper functions\n",
    "def copy_model(model):\n",
    "    return copy.deepcopy(model)\n",
    "\n",
    "def cosine_similarity(x, y):\n",
    "    return F.cosine_similarity(x.view(1, -1), y.view(1, -1))\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    noise = torch.rand_like(logits)\n",
    "    gumbel = -torch.log(-torch.log(noise + 1e-20) + 1e-20)\n",
    "    return torch.sigmoid((logits + gumbel) / temperature)\n",
    "\n",
    "def kl_divergence(p, prior):\n",
    "    p = torch.clamp(p, 1e-6, 1 - 1e-6)\n",
    "    prior = torch.full_like(p, prior)\n",
    "    return (p * (p / prior).log() + (1 - p) * ((1 - p) / (1 - prior)).log()).mean()\n",
    "\n",
    "def apply_mask_to_model(model, mask, config):\n",
    "    offset = 0\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            numel = param.numel()\n",
    "            param.data *= mask[offset:offset+numel].view(param.shape)\n",
    "            offset += numel\n",
    "    print(\"✅ Mask applied to model weights.\")\n",
    "\n",
    "def train_classifier(model, loader, config):\n",
    "    model.train()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config.lr, momentum=config.momentum)\n",
    "    for epoch in range(config.epochs):\n",
    "        total_loss = 0\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(config.device), target.to(config.device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"[Classifier Epoch {epoch+1}] Loss: {total_loss:.4f}\")\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(config.device), target.to(config.device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    acc = correct / len(loader.dataset)\n",
    "    print(f\"🎯 Evaluation Accuracy: {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "def is_unlearning_feasible_margin(model, x, y, threshold=0.0):\n",
    "    output = model(x.unsqueeze(0))\n",
    "    probs = F.softmax(output, dim=1)\n",
    "    top2 = torch.topk(probs, 2).values.squeeze()\n",
    "    margin = top2[0] - top2[1]\n",
    "    print(f\"→ Margin: {margin.item():.4f} | Feasible: {'Yes' if margin > threshold else 'No'}\")\n",
    "    return margin > threshold\n",
    "\n",
    "def check_mask_overlap(new_mask, previous_masks, threshold=0.9):\n",
    "    if not previous_masks:\n",
    "        return True\n",
    "    for i, prev in enumerate(previous_masks):\n",
    "        sim = cosine_similarity(new_mask, prev).item()\n",
    "        print(f\"→ Cosine similarity with previous mask {i}: {sim:.4f}\")\n",
    "        if sim > threshold:\n",
    "            print(\"⛔ Overlap too high. Rejecting mask.\\n\")\n",
    "            return False\n",
    "    print(\"✅ Mask is unique (overlap below threshold).\\n\")\n",
    "    return True\n",
    "\n",
    "def build_graph_from_model(model):\n",
    "    sizes = [param.numel() for param in model.parameters()]\n",
    "    total_params = sum(sizes)\n",
    "    config.num_params = total_params\n",
    "    x = torch.ones((total_params, 1)).to(config.device)\n",
    "    edge_index = []\n",
    "    idx = 0\n",
    "    for size in sizes:\n",
    "        for i in range(size - 1):\n",
    "            edge_index.append([idx + i, idx + i + 1])\n",
    "            edge_index.append([idx + i + 1, idx + i])\n",
    "        idx += size\n",
    "    edge_index = torch.tensor(edge_index).t().contiguous().to(config.device)\n",
    "    config.edge_index = edge_index\n",
    "    print(f\"Graph built with {total_params} nodes and {edge_index.shape[1]} edges.\")\n",
    "\n",
    "def train_mask_generator(mask_net, classifier, forget_data, config):\n",
    "    mask_net.train()\n",
    "    optimizer = torch.optim.Adam(mask_net.parameters(), lr=config.mask_lr)\n",
    "\n",
    "    x_forget, y_forget = forget_data\n",
    "    x_forget = x_forget.to(config.device).unsqueeze(0)\n",
    "    y_forget = y_forget.to(config.device)\n",
    "\n",
    "    x_init = torch.ones(config.num_params, 1).to(config.device)\n",
    "    edge_index = config.edge_index.to(config.device)\n",
    "\n",
    "    print(f\"\\n🔧 Starting GCN training to forget data point (label: {y_forget.item()})\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(config.mask_epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        mask_logits = mask_net(x_init, edge_index).squeeze()\n",
    "        mask_probs = torch.sigmoid(mask_logits)\n",
    "        mask_sampled = gumbel_softmax_sample(mask_logits, temperature=config.temperature)\n",
    "\n",
    "        temp_model = copy_model(classifier).to(config.device)\n",
    "        apply_mask_to_model(temp_model, mask_sampled, config)\n",
    "\n",
    "        output = temp_model(x_forget)\n",
    "        ce_loss = F.cross_entropy(output, y_forget.unsqueeze(0))\n",
    "        kl = kl_divergence(mask_probs, config.prior)\n",
    "\n",
    "        loss = ce_loss + config.kl_coeff * kl\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"[Epoch {epoch+1:3d}/{config.mask_epochs}] Loss: {loss.item():.4f} | CE: {ce_loss.item():.4f} | KL: {kl.item():.4f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"✅ GCN training complete in {end_time - start_time:.2f}s\\n\")\n",
    "    return mask_logits.detach()\n",
    "\n",
    "def main():\n",
    "    print(\"Starting training process...\")\n",
    "    classifier = SimpleClassifier().to(config.device)\n",
    "    train_classifier(classifier, train_loader, config)\n",
    "\n",
    "    acc_before = evaluate_model(classifier, test_loader)\n",
    "    print(f\"\\nBefore Unlearning:\\nAccuracy: {acc_before:.4f}\")\n",
    "\n",
    "    build_graph_from_model(classifier)\n",
    "\n",
    "    accepted_masks = []\n",
    "    forget_x, forget_y, forget_idx, forget_mask = None, None, None, None\n",
    "\n",
    "    for attempt in range(1, config.max_attempts + 1):\n",
    "        idx = 80  # random.randint(0, len(train_dataset) - 1) for random point\n",
    "        x_i, y_i = train_dataset[idx]\n",
    "        x_i = x_i.to(config.device)\n",
    "        y_i = torch.tensor(y_i).to(config.device)\n",
    "        print(f\"\\nAttempt {attempt}: Trying forget point idx {idx} with label {y_i.item()}\")\n",
    "\n",
    "        if not is_unlearning_feasible_margin(classifier, x_i, y_i):\n",
    "            print(\"❌ Not feasible by margin, trying another point...\")\n",
    "            continue\n",
    "\n",
    "        print(\"✅ Forget point is feasible for unlearning.\")\n",
    "\n",
    "        mask_net = GCNMaskNet(1).to(config.device)\n",
    "        mask_logits = train_mask_generator(mask_net, classifier, (x_i, y_i), config)\n",
    "\n",
    "        if not check_mask_overlap(mask_logits, accepted_masks, config.overlap_threshold):\n",
    "            print(\"❌ Mask overlap too high, trying next point...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"✅ Forget point idx {idx} accepted for unlearning.\")\n",
    "        accepted_masks.append(mask_logits)\n",
    "        forget_x, forget_y, forget_idx, forget_mask = x_i, y_i, idx, mask_logits\n",
    "        break\n",
    "    else:\n",
    "        print(\"❌ No feasible forget point found within max attempts passing overlap checks.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nApplying mask to classifier weights for unlearning...\")\n",
    "\n",
    "    # Confidence BEFORE unlearning (per class)\n",
    "    output_before = classifier(forget_x.unsqueeze(0))\n",
    "    probs_before = F.softmax(output_before, dim=1).squeeze().tolist()\n",
    "    print(f\"\\n📊 Confidence BEFORE unlearning on forget point (label {forget_y.item()}):\")\n",
    "    for cls, prob in enumerate(probs_before):\n",
    "        print(f\"  Class {cls}: {prob:.4f}\")\n",
    "    conf_before = probs_before[forget_y.item()]\n",
    "    print(f\"→ Target Class {forget_y.item()} Confidence: {conf_before:.4f}\\n\")\n",
    "\n",
    "    apply_mask_to_model(classifier, forget_mask, config)\n",
    "\n",
    "    # Confidence AFTER unlearning (per class)\n",
    "    output_after = classifier(forget_x.unsqueeze(0))\n",
    "    probs_after = F.softmax(output_after, dim=1).squeeze().tolist()\n",
    "    print(f\"\\n📊 Confidence AFTER unlearning on forget point (label {forget_y.item()}):\")\n",
    "    for cls, prob in enumerate(probs_after):\n",
    "        print(f\"  Class {cls}: {prob:.4f}\")\n",
    "    conf_after = probs_after[forget_y.item()]\n",
    "    print(f\"→ Target Class {forget_y.item()} Confidence: {conf_after:.4f}\\n\")\n",
    "\n",
    "    acc_after = evaluate_model(classifier, test_loader)\n",
    "    print(f\"\\nAfter Unlearning:\\nAccuracy: {acc_after:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c9fd1-8aa5-485a-bcd8-995b35f60f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
