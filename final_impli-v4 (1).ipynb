{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aafd3410-e2a9-4060-969a-2563b9419b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier...\n",
      "[Classifier] Epoch 1, Loss=165.5215\n",
      "[Classifier] Epoch 2, Loss=62.5292\n",
      "Accuracy before unlearning:\n",
      "Accuracy: 0.9678\n",
      "\n",
      "Forget point confidences BEFORE unlearning:\n",
      "Forget point 0 confidence before unlearning: 0.9921\n",
      "Forget point 1 confidence before unlearning: 0.9999\n",
      "Forget point 2 confidence before unlearning: 0.9997\n",
      "Forget point 3 confidence before unlearning: 0.9086\n",
      "Forget point 4 confidence before unlearning: 0.8934\n",
      "Forget point 5 confidence before unlearning: 0.9430\n",
      "Forget point 6 confidence before unlearning: 0.9999\n",
      "Forget point 7 confidence before unlearning: 0.9929\n",
      "Forget point 8 confidence before unlearning: 0.9998\n",
      "Forget point 9 confidence before unlearning: 0.9911\n",
      "Forget point 10 confidence before unlearning: 0.9949\n",
      "Forget point 11 confidence before unlearning: 0.9973\n",
      "Forget point 12 confidence before unlearning: 0.9990\n",
      "Forget point 13 confidence before unlearning: 0.9872\n",
      "Forget point 14 confidence before unlearning: 0.9887\n",
      "Forget point 15 confidence before unlearning: 0.9994\n",
      "Forget point 16 confidence before unlearning: 0.9998\n",
      "Forget point 17 confidence before unlearning: 0.9774\n",
      "Forget point 18 confidence before unlearning: 0.8442\n",
      "Forget point 19 confidence before unlearning: 0.9996\n",
      "Forget point 20 confidence before unlearning: 0.5431\n",
      "Forget point 21 confidence before unlearning: 0.9987\n",
      "Forget point 22 confidence before unlearning: 0.9916\n",
      "Forget point 23 confidence before unlearning: 0.9996\n",
      "Forget point 24 confidence before unlearning: 0.9976\n",
      "Forget point 25 confidence before unlearning: 0.9993\n",
      "Forget point 26 confidence before unlearning: 0.9986\n",
      "Forget point 27 confidence before unlearning: 0.9917\n",
      "Forget point 28 confidence before unlearning: 0.8983\n",
      "Forget point 29 confidence before unlearning: 0.9918\n",
      "Forget point 30 confidence before unlearning: 0.0080\n",
      "Forget point 31 confidence before unlearning: 0.9992\n",
      "Forget point 32 confidence before unlearning: 1.0000\n",
      "Forget point 33 confidence before unlearning: 0.9965\n",
      "Forget point 34 confidence before unlearning: 0.9932\n",
      "Forget point 35 confidence before unlearning: 0.9949\n",
      "Forget point 36 confidence before unlearning: 0.9737\n",
      "Forget point 37 confidence before unlearning: 0.9969\n",
      "Forget point 38 confidence before unlearning: 0.9997\n",
      "Forget point 39 confidence before unlearning: 0.9997\n",
      "Forget point 40 confidence before unlearning: 1.0000\n",
      "Forget point 41 confidence before unlearning: 0.9985\n",
      "Forget point 42 confidence before unlearning: 0.9953\n",
      "Forget point 43 confidence before unlearning: 0.9996\n",
      "Forget point 44 confidence before unlearning: 0.9996\n",
      "Forget point 45 confidence before unlearning: 0.9999\n",
      "Forget point 46 confidence before unlearning: 0.9985\n",
      "Forget point 47 confidence before unlearning: 0.9991\n",
      "Forget point 48 confidence before unlearning: 0.9980\n",
      "Forget point 49 confidence before unlearning: 0.8222\n",
      "\n",
      "Iterative Refinement Step 1/3\n",
      "[MaskNet] Epoch 1, Loss=20.0843\n",
      "[MaskNet] Epoch 2, Loss=19.8416\n",
      "Accuracy: 0.4985\n",
      "Mean forget point confidence after masking: 0.3709\n",
      "\n",
      "Iterative Refinement Step 2/3\n",
      "[MaskNet] Epoch 1, Loss=20.6920\n",
      "[MaskNet] Epoch 2, Loss=20.4992\n",
      "Accuracy: 0.5920\n",
      "Mean forget point confidence after masking: 0.1860\n",
      "\n",
      "Iterative Refinement Step 3/3\n",
      "[MaskNet] Epoch 1, Loss=19.8794\n",
      "[MaskNet] Epoch 2, Loss=20.2345\n",
      "Accuracy: 0.4852\n",
      "Mean forget point confidence after masking: 0.3892\n",
      "No good mask found, using last mask logits.\n",
      "Applying final mask to classifier...\n",
      "Accuracy after unlearning:\n",
      "Accuracy: 0.9668\n",
      "\n",
      "Forget point confidences AFTER unlearning:\n",
      "Forget point 0 confidence after unlearning: 0.5337\n",
      "Forget point 1 confidence after unlearning: 0.7222\n",
      "Forget point 2 confidence after unlearning: 0.7108\n",
      "Forget point 3 confidence after unlearning: 0.3443\n",
      "Forget point 4 confidence after unlearning: 0.3635\n",
      "Forget point 5 confidence after unlearning: 0.4477\n",
      "Forget point 6 confidence after unlearning: 0.7412\n",
      "Forget point 7 confidence after unlearning: 0.5650\n",
      "Forget point 8 confidence after unlearning: 0.7653\n",
      "Forget point 9 confidence after unlearning: 0.4420\n",
      "Forget point 10 confidence after unlearning: 0.5035\n",
      "Forget point 11 confidence after unlearning: 0.5337\n",
      "Forget point 12 confidence after unlearning: 0.6530\n",
      "Forget point 13 confidence after unlearning: 0.4695\n",
      "Forget point 14 confidence after unlearning: 0.4282\n",
      "Forget point 15 confidence after unlearning: 0.6615\n",
      "Forget point 16 confidence after unlearning: 0.7333\n",
      "Forget point 17 confidence after unlearning: 0.3675\n",
      "Forget point 18 confidence after unlearning: 0.2766\n",
      "Forget point 19 confidence after unlearning: 0.6350\n",
      "Forget point 20 confidence after unlearning: 0.2867\n",
      "Forget point 21 confidence after unlearning: 0.6004\n",
      "Forget point 22 confidence after unlearning: 0.4420\n",
      "Forget point 23 confidence after unlearning: 0.6964\n",
      "Forget point 24 confidence after unlearning: 0.6049\n",
      "Forget point 25 confidence after unlearning: 0.5883\n",
      "Forget point 26 confidence after unlearning: 0.6378\n",
      "Forget point 27 confidence after unlearning: 0.4347\n",
      "Forget point 28 confidence after unlearning: 0.3565\n",
      "Forget point 29 confidence after unlearning: 0.5359\n",
      "Forget point 30 confidence after unlearning: 0.0863\n",
      "Forget point 31 confidence after unlearning: 0.6174\n",
      "Forget point 32 confidence after unlearning: 0.8644\n",
      "Forget point 33 confidence after unlearning: 0.5471\n",
      "Forget point 34 confidence after unlearning: 0.6096\n",
      "Forget point 35 confidence after unlearning: 0.5492\n",
      "Forget point 36 confidence after unlearning: 0.3876\n",
      "Forget point 37 confidence after unlearning: 0.6475\n",
      "Forget point 38 confidence after unlearning: 0.6331\n",
      "Forget point 39 confidence after unlearning: 0.7005\n",
      "Forget point 40 confidence after unlearning: 0.8060\n",
      "Forget point 41 confidence after unlearning: 0.6127\n",
      "Forget point 42 confidence after unlearning: 0.4953\n",
      "Forget point 43 confidence after unlearning: 0.6238\n",
      "Forget point 44 confidence after unlearning: 0.6371\n",
      "Forget point 45 confidence after unlearning: 0.7674\n",
      "Forget point 46 confidence after unlearning: 0.5867\n",
      "Forget point 47 confidence after unlearning: 0.6428\n",
      "Forget point 48 confidence after unlearning: 0.6010\n",
      "Forget point 49 confidence after unlearning: 0.3329\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch_geometric.data import Data as GeoData\n",
    "from torch_geometric.nn import GCNConv\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Fix random seeds for reproducibility\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# --- Config ---\n",
    "class Config:\n",
    "    batch_size = 128\n",
    "    lr = 0.001\n",
    "    epochs = 2  # classifier epochs\n",
    "    mask_epochs = 2  # mask training epochs per refinement\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    mask_threshold = 0.5\n",
    "    iterative_refinement_steps = 3\n",
    "    mask_layers = ['fc2.weight', 'fc3.weight']  # last two layers for masking\n",
    "    kl_weight = 0.1  # weight of KL divergence regularization\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# --- Load MNIST ---\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "# Define subset for forget indices (example: 50 points after some index)\n",
    "forget_indices = list(range(50, 100))\n",
    "forget_data = Subset(train_dataset, forget_indices)\n",
    "\n",
    "# --- Classifier ---\n",
    "\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# --- Mask Generator GCN for multiple layers ---\n",
    "\n",
    "class MaskGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 64)\n",
    "        self.conv2 = GCNConv(64, 32)\n",
    "        self.conv3 = GCNConv(32, 1)  # output logits per node\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index.to(data.x.device)\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        logits = self.conv3(x, edge_index).squeeze(-1)  # shape [num_nodes]\n",
    "        return logits\n",
    "\n",
    "# --- Build Param Graph with same output node connections ---\n",
    "\n",
    "def build_param_graph_same_output(model):\n",
    "    param_tensors = []\n",
    "    param_shapes = []\n",
    "    param_sizes = []\n",
    "    for layer in config.mask_layers:\n",
    "        param = dict(model.named_parameters())[layer]\n",
    "        param_shapes.append(param.shape)\n",
    "        flat_param = param.view(-1).detach().cpu().unsqueeze(1)\n",
    "        param_tensors.append(flat_param)\n",
    "        param_sizes.append(flat_param.size(0))\n",
    "\n",
    "    x = torch.cat(param_tensors, dim=0)\n",
    "    cum_sizes = np.cumsum([0] + param_sizes)\n",
    "\n",
    "    edge_src = []\n",
    "    edge_dst = []\n",
    "\n",
    "    # Layer 0: fc2.weight, shape = (128, 256)\n",
    "    out0, in0 = param_shapes[0]\n",
    "\n",
    "    # Layer 1: fc3.weight, shape = (10, 128)\n",
    "    out1, in1 = param_shapes[1]\n",
    "\n",
    "    # Within fc2.weight: connect weights with same output neuron (same row)\n",
    "    start0 = cum_sizes[0]\n",
    "    for out_neuron in range(out0):\n",
    "        row_start = start0 + out_neuron * in0\n",
    "        for i in range(in0 - 1):\n",
    "            src = row_start + i\n",
    "            dst = row_start + i + 1\n",
    "            edge_src.append(src)\n",
    "            edge_dst.append(dst)\n",
    "            edge_src.append(dst)\n",
    "            edge_dst.append(src)\n",
    "\n",
    "    # Within fc3.weight: connect weights with same output neuron (same row)\n",
    "    start1 = cum_sizes[1]\n",
    "    for out_neuron in range(out1):\n",
    "        row_start = start1 + out_neuron * in1\n",
    "        for i in range(in1 - 1):\n",
    "            src = row_start + i\n",
    "            dst = row_start + i + 1\n",
    "            edge_src.append(src)\n",
    "            edge_dst.append(dst)\n",
    "            edge_src.append(dst)\n",
    "            edge_dst.append(src)\n",
    "\n",
    "    # Between fc2.weight and fc3.weight:\n",
    "    # connect weights corresponding to same neuron index at boundary:\n",
    "    # for each neuron j in fc2 output (0..127),\n",
    "    # connect all weights in fc2 row j to all weights in fc3 column j\n",
    "\n",
    "    for j in range(in1):  # in1 == 128\n",
    "        fc2_row_start = start0 + j * in0\n",
    "        fc2_nodes = list(range(fc2_row_start, fc2_row_start + in0))\n",
    "\n",
    "        fc3_nodes = [start1 + i * in1 + j for i in range(out1)]\n",
    "\n",
    "        for src in fc2_nodes:\n",
    "            for dst in fc3_nodes:\n",
    "                edge_src.append(src)\n",
    "                edge_dst.append(dst)\n",
    "                edge_src.append(dst)\n",
    "                edge_dst.append(src)\n",
    "\n",
    "    edge_index = torch.tensor([edge_src, edge_dst], dtype=torch.long)\n",
    "    data = GeoData(x=x, edge_index=edge_index)\n",
    "    return data, cum_sizes\n",
    "\n",
    "# --- Gumbel Sigmoid for soft/hard mask reparameterization ---\n",
    "\n",
    "def gumbel_sigmoid(logits, tau=1.0, hard=False, eps=1e-10):\n",
    "    gumbels = -torch.empty_like(logits).exponential_().log()\n",
    "    y = (logits + gumbels) / tau\n",
    "    y = torch.sigmoid(y)\n",
    "    if hard:\n",
    "        y_hard = (y > 0.5).float()\n",
    "        y = (y_hard - y).detach() + y\n",
    "    return y\n",
    "\n",
    "# --- KL divergence for Bernoulli (mask logits q vs prior p) ---\n",
    "\n",
    "def bernoulli_kl(logits, prior=0.5):\n",
    "    q = torch.sigmoid(logits)\n",
    "    p = torch.tensor(prior, device=logits.device)\n",
    "    kl = q * (torch.log(q + 1e-10) - torch.log(p + 1e-10)) + \\\n",
    "         (1 - q) * (torch.log(1 - q + 1e-10) - torch.log(1 - p + 1e-10))\n",
    "    return kl.sum()\n",
    "\n",
    "# --- Apply mask logits to classifier parameters ---\n",
    "\n",
    "def apply_mask_logits_to_classifier(classifier, mask_logits, cum_sizes):\n",
    "    mask_probs = torch.sigmoid(mask_logits).detach().cpu()\n",
    "    with torch.no_grad():\n",
    "        for idx, layer_name in enumerate(config.mask_layers):\n",
    "            param = dict(classifier.named_parameters())[layer_name]\n",
    "            start = cum_sizes[idx]\n",
    "            end = cum_sizes[idx + 1]\n",
    "            mask_layer = mask_probs[start:end].view(param.shape).to(param.device)\n",
    "            param.mul_(mask_layer)\n",
    "\n",
    "# --- Evaluate confidence on a data point ---\n",
    "\n",
    "def get_confidence(model, data_point, device):\n",
    "    model.eval()\n",
    "    x, y = data_point\n",
    "    x = x.unsqueeze(0).to(device)  # add batch dim\n",
    "    with torch.no_grad():\n",
    "        output = model(x)\n",
    "        probs = F.softmax(output, dim=1)\n",
    "    confidence = probs[0, y].item()\n",
    "    return confidence\n",
    "\n",
    "# --- Evaluate accuracy on a dataloader ---\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            preds = output.argmax(dim=1)\n",
    "            correct += (preds == target).sum().item()\n",
    "            total += target.size(0)\n",
    "    acc = correct / total\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "# --- Train classifier ---\n",
    "\n",
    "def train_classifier(model, loader, device):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(config.epochs):\n",
    "        total_loss = 0\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"[Classifier] Epoch {epoch+1}, Loss={total_loss:.4f}\")\n",
    "\n",
    "# --- Train mask generator ---\n",
    "\n",
    "def train_mask_generator(mask_net, classifier, forget_data, idx_map, config):\n",
    "    mask_net.train()\n",
    "    optimizer = optim.Adam(mask_net.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    prior_prob = 0.5\n",
    "\n",
    "    for epoch in range(config.mask_epochs):\n",
    "        total_loss = 0\n",
    "        for x, y in forget_data:\n",
    "            x = x.unsqueeze(0).to(config.device)\n",
    "            y_tensor = torch.tensor([y], dtype=torch.long).to(config.device)\n",
    "\n",
    "            data_graph, _ = build_param_graph_same_output(classifier)\n",
    "            data_graph = data_graph.to(config.device)\n",
    "\n",
    "            logits = mask_net(data_graph)\n",
    "            mask = gumbel_sigmoid(logits, tau=1.0, hard=False)\n",
    "\n",
    "            temp_model = MNISTClassifier().to(config.device)\n",
    "            temp_model.load_state_dict(classifier.state_dict())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                start_idx = 0\n",
    "                for idx, layer_name in enumerate(config.mask_layers):\n",
    "                    param = dict(temp_model.named_parameters())[layer_name]\n",
    "                    size = param.numel()\n",
    "                    layer_mask = mask[start_idx:start_idx + size].view(param.shape)\n",
    "                    param.mul_(layer_mask)\n",
    "                    start_idx += size\n",
    "\n",
    "            temp_model.eval()\n",
    "            output = temp_model(x)\n",
    "            ce_loss = criterion(output, y_tensor)\n",
    "\n",
    "            kl_loss = bernoulli_kl(logits, prior_prob)\n",
    "\n",
    "            loss = ce_loss + config.kl_weight * kl_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"[MaskNet] Epoch {epoch+1}, Loss={total_loss:.4f}\")\n",
    "\n",
    "# --- Iterative refinement ---\n",
    "\n",
    "def iterative_refinement(classifier, mask_net, forget_data, idx_map, config):\n",
    "    best_mask_logits = None\n",
    "    best_acc = 0\n",
    "\n",
    "    for iteration in range(config.iterative_refinement_steps):\n",
    "        print(f\"\\nIterative Refinement Step {iteration+1}/{config.iterative_refinement_steps}\")\n",
    "\n",
    "        train_mask_generator(mask_net, classifier, forget_data, idx_map, config)\n",
    "\n",
    "        data_graph, cum_sizes = build_param_graph_same_output(classifier)\n",
    "        data_graph = data_graph.to(config.device)\n",
    "\n",
    "        mask_logits = mask_net(data_graph)\n",
    "\n",
    "        hard_mask = (torch.sigmoid(mask_logits) > config.mask_threshold).float()\n",
    "\n",
    "        temp_model = MNISTClassifier().to(config.device)\n",
    "        temp_model.load_state_dict(classifier.state_dict())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            start_idx = 0\n",
    "            for idx, layer_name in enumerate(config.mask_layers):\n",
    "                param = dict(temp_model.named_parameters())[layer_name]\n",
    "                size = param.numel()\n",
    "                layer_mask = hard_mask[start_idx:start_idx+size].view(param.shape).to(param.device)\n",
    "                param.mul_(layer_mask)\n",
    "                start_idx += size\n",
    "\n",
    "        train_acc = evaluate_model(temp_model, train_loader, config.device)\n",
    "\n",
    "        confs = []\n",
    "        for data_point in forget_data:\n",
    "            conf = get_confidence(temp_model, data_point, config.device)\n",
    "            confs.append(conf)\n",
    "        mean_conf = np.mean(confs)\n",
    "        print(f\"Mean forget point confidence after masking: {mean_conf:.4f}\")\n",
    "\n",
    "        if train_acc >= best_acc and mean_conf < 0.1:\n",
    "            best_acc = train_acc\n",
    "            best_mask_logits = mask_logits.detach()\n",
    "\n",
    "        if best_mask_logits is not None and best_acc >= 0.95 and mean_conf < 0.1:\n",
    "            print(\"Stopping early: good mask found.\")\n",
    "            break\n",
    "\n",
    "    return best_mask_logits, cum_sizes\n",
    "\n",
    "# --- Main pipeline ---\n",
    "\n",
    "def main():\n",
    "    classifier = MNISTClassifier().to(config.device)\n",
    "    print(\"Training classifier...\")\n",
    "    train_classifier(classifier, train_loader, config.device)\n",
    "    print(\"Accuracy before unlearning:\")\n",
    "    evaluate_model(classifier, test_loader, config.device)\n",
    "\n",
    "    # Print forget point confidences BEFORE unlearning\n",
    "    print(\"\\nForget point confidences BEFORE unlearning:\")\n",
    "    for i, data_point in enumerate(forget_data):\n",
    "        conf = get_confidence(classifier, data_point, config.device)\n",
    "        print(f\"Forget point {i} confidence before unlearning: {conf:.4f}\")\n",
    "\n",
    "    idx_map = None\n",
    "    mask_net = MaskGenerator(in_channels=1).to(config.device)\n",
    "\n",
    "    best_mask_logits, cum_sizes = iterative_refinement(classifier, mask_net, forget_data, idx_map, config)\n",
    "\n",
    "    if best_mask_logits is None:\n",
    "        print(\"No good mask found, using last mask logits.\")\n",
    "        data_graph, cum_sizes = build_param_graph_same_output(classifier)\n",
    "        best_mask_logits = mask_net(data_graph.to(config.device)).detach()\n",
    "\n",
    "    print(\"Applying final mask to classifier...\")\n",
    "    apply_mask_logits_to_classifier(classifier, best_mask_logits, cum_sizes)\n",
    "\n",
    "    print(\"Accuracy after unlearning:\")\n",
    "    evaluate_model(classifier, test_loader, config.device)\n",
    "\n",
    "    print(\"\\nForget point confidences AFTER unlearning:\")\n",
    "    for i, data_point in enumerate(forget_data):\n",
    "        conf = get_confidence(classifier, data_point, config.device)\n",
    "        print(f\"Forget point {i} confidence after unlearning: {conf:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ccc19ef-ec0b-4246-937f-1877f995c06e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Classifier] Epoch 1/10 - Loss: 113.5418\n",
      "[Classifier] Epoch 2/10 - Loss: 56.1859\n",
      "[Classifier] Epoch 3/10 - Loss: 46.8765\n",
      "[Classifier] Epoch 4/10 - Loss: 43.9637\n",
      "[Classifier] Epoch 5/10 - Loss: 36.4129\n",
      "[Classifier] Epoch 6/10 - Loss: 31.2952\n",
      "[Classifier] Epoch 7/10 - Loss: 35.9336\n",
      "[Classifier] Epoch 8/10 - Loss: 33.2211\n",
      "[Classifier] Epoch 9/10 - Loss: 29.2238\n",
      "[Classifier] Epoch 10/10 - Loss: 28.1778\n",
      "Before Unlearning:\n",
      "Test Accuracy: 0.9709\n",
      "Forgetting data index 5, label 2\n",
      "Confidence of forget point before unlearning: 1.0000\n",
      "Initial test accuracy: 0.9709\n",
      "[MaskNet] Epoch 1/30 - Loss: 1.882084 - ForgetConf: 0.848138 - TestAcc: 0.9520\n",
      "[MaskNet] Epoch 2/30 - Loss: 1.909341 - ForgetConf: 0.817752 - TestAcc: 0.9490\n",
      "[MaskNet] Epoch 3/30 - Loss: 1.904680 - ForgetConf: 0.820161 - TestAcc: 0.9491\n",
      "[MaskNet] Epoch 4/30 - Loss: 1.889951 - ForgetConf: 0.828629 - TestAcc: 0.9499\n",
      "[MaskNet] Epoch 5/30 - Loss: 1.876714 - ForgetConf: 0.836931 - TestAcc: 0.9505\n",
      "[MaskNet] Epoch 6/30 - Loss: 1.868831 - ForgetConf: 0.843257 - TestAcc: 0.9510\n",
      "[MaskNet] Epoch 7/30 - Loss: 1.865257 - ForgetConf: 0.847237 - TestAcc: 0.9517\n",
      "[MaskNet] Epoch 8/30 - Loss: 1.864295 - ForgetConf: 0.848597 - TestAcc: 0.9517\n",
      "[MaskNet] Epoch 9/30 - Loss: 1.864890 - ForgetConf: 0.847881 - TestAcc: 0.9517\n",
      "[MaskNet] Epoch 10/30 - Loss: 1.866704 - ForgetConf: 0.846013 - TestAcc: 0.9517\n",
      "[MaskNet] Epoch 11/30 - Loss: 1.869228 - ForgetConf: 0.843881 - TestAcc: 0.9512\n",
      "[MaskNet] Epoch 12/30 - Loss: 1.871513 - ForgetConf: 0.842146 - TestAcc: 0.9511\n",
      "[MaskNet] Epoch 13/30 - Loss: 1.872687 - ForgetConf: 0.841165 - TestAcc: 0.9508\n",
      "[MaskNet] Epoch 14/30 - Loss: 1.872529 - ForgetConf: 0.840977 - TestAcc: 0.9508\n",
      "[MaskNet] Epoch 15/30 - Loss: 1.871483 - ForgetConf: 0.841387 - TestAcc: 0.9508\n",
      "[MaskNet] Epoch 16/30 - Loss: 1.870227 - ForgetConf: 0.842107 - TestAcc: 0.9509\n",
      "[MaskNet] Epoch 17/30 - Loss: 1.869313 - ForgetConf: 0.842808 - TestAcc: 0.9510\n",
      "[MaskNet] Epoch 18/30 - Loss: 1.868962 - ForgetConf: 0.843220 - TestAcc: 0.9511\n",
      "[MaskNet] Epoch 19/30 - Loss: 1.869135 - ForgetConf: 0.843184 - TestAcc: 0.9511\n",
      "[MaskNet] Epoch 20/30 - Loss: 1.869717 - ForgetConf: 0.842677 - TestAcc: 0.9510\n",
      "[MaskNet] Epoch 21/30 - Loss: 1.870632 - ForgetConf: 0.841801 - TestAcc: 0.9509\n",
      "[MaskNet] Epoch 22/30 - Loss: 1.871803 - ForgetConf: 0.840736 - TestAcc: 0.9508\n",
      "[MaskNet] Epoch 23/30 - Loss: 1.873058 - ForgetConf: 0.839705 - TestAcc: 0.9507\n",
      "[MaskNet] Epoch 24/30 - Loss: 1.874114 - ForgetConf: 0.838917 - TestAcc: 0.9505\n",
      "[MaskNet] Epoch 25/30 - Loss: 1.874688 - ForgetConf: 0.838515 - TestAcc: 0.9505\n",
      "[MaskNet] Epoch 26/30 - Loss: 1.874635 - ForgetConf: 0.838551 - TestAcc: 0.9505\n",
      "[MaskNet] Epoch 27/30 - Loss: 1.874027 - ForgetConf: 0.838972 - TestAcc: 0.9505\n",
      "[MaskNet] Epoch 28/30 - Loss: 1.873095 - ForgetConf: 0.839653 - TestAcc: 0.9507\n",
      "[MaskNet] Epoch 29/30 - Loss: 1.872109 - ForgetConf: 0.840427 - TestAcc: 0.9507\n",
      "[MaskNet] Epoch 30/30 - Loss: 1.871274 - ForgetConf: 0.841127 - TestAcc: 0.9508\n",
      "After Unlearning:\n",
      "Test Accuracy: 0.9508\n",
      "Confidence of forget point after unlearning: 0.8411\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch_geometric.data import Data as GeoData\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Config\n",
    "class Config:\n",
    "    batch_size = 128\n",
    "    epochs = 2\n",
    "    lr = 0.01\n",
    "    mask_lr = 0.005\n",
    "    mask_epochs = 5  # increased mask training epochs\n",
    "    kl_coeff = 0.1\n",
    "    l1_coeff = 1e-3  # L1 regularization weight for mask sparsity\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Seed\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Model\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# GCN Mask Generator\n",
    "class MaskNet(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        return torch.sigmoid(self.conv2(x, edge_index))\n",
    "\n",
    "# Dataset\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.MNIST(root=\".\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=\".\", train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "# Helpers\n",
    "def evaluate_model(model, loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    acc = correct / total\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "def get_forget_point():\n",
    "    for i, (x, y) in enumerate(train_dataset):\n",
    "        if y == 2:  # Example target digit\n",
    "            return i, x.unsqueeze(0), y\n",
    "\n",
    "# Build param graph for last two layers\n",
    "layer_shapes = {\n",
    "    'fc2.weight': (128, 256),\n",
    "    'fc2.bias': (128,),\n",
    "    'fc3.weight': (10, 128),\n",
    "    'fc3.bias': (10,)\n",
    "}\n",
    "\n",
    "def build_param_graph(model):\n",
    "    param_nodes, edge_index, features = [], [], []\n",
    "    idx_map, reverse_map = {}, {}\n",
    "    idx = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if name in layer_shapes:\n",
    "            param_nodes.append(param.detach().view(-1))\n",
    "            idx_map[name] = (idx, idx + param.numel())\n",
    "            for i in range(param.numel()):\n",
    "                reverse_map[idx + i] = name\n",
    "            idx += param.numel()\n",
    "\n",
    "    param_tensor = torch.cat(param_nodes)\n",
    "    \n",
    "    # Connect nodes only if they belong to same output neuron (for weights)\n",
    "    def connect_same_output(shape, start_idx):\n",
    "        edges = []\n",
    "        if len(shape) == 2:\n",
    "            for i in range(shape[0]):\n",
    "                base = start_idx + i * shape[1]\n",
    "                # fully connect weights in the same output neuron row (excluding self loops)\n",
    "                edges += [[base + j, base + k] for j in range(shape[1]) for k in range(shape[1]) if j != k]\n",
    "        else:\n",
    "            # bias: no connections\n",
    "            pass\n",
    "        return edges\n",
    "\n",
    "    edge_index = []\n",
    "    for name, (start, end) in idx_map.items():\n",
    "        shape = model.state_dict()[name].shape\n",
    "        edge_index += connect_same_output(shape, start)\n",
    "\n",
    "    if len(edge_index) == 0:\n",
    "        # To avoid error if no edges (e.g., only biases)\n",
    "        edge_index = torch.empty((2,0), dtype=torch.long)\n",
    "    else:\n",
    "        edge_index = torch.tensor(edge_index).t().contiguous()\n",
    "\n",
    "    return GeoData(x=param_tensor.view(-1, 1), edge_index=edge_index), param_tensor.view(-1, 1).shape[0]\n",
    "\n",
    "# KL divergence\n",
    "def kl_divergence(q):\n",
    "    eps = 1e-8\n",
    "    p = torch.full_like(q, 0.5)\n",
    "    q = torch.clamp(q, eps, 1 - eps)\n",
    "    return (q * torch.log(q / p) + (1 - q) * torch.log((1 - q) / (1 - p))).mean()\n",
    "\n",
    "def apply_mask_to_weights(model, mask_tensor):\n",
    "    start = 0\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in layer_shapes:\n",
    "                length = param.numel()\n",
    "                mask = mask_tensor[start:start+length].view(param.shape)\n",
    "                param.mul_(mask)\n",
    "                start += length\n",
    "\n",
    "# Clone model utility\n",
    "def clone_model_with_mask(model, mask):\n",
    "    new_model = Classifier().to(config.device)\n",
    "    new_model.load_state_dict(model.state_dict())\n",
    "    apply_mask_to_weights(new_model, mask)\n",
    "    return new_model\n",
    "\n",
    "# Training Mask Generator\n",
    "def train_mask_generator(mask_net, model, forget_data, device, config):\n",
    "    optimizer = optim.Adam(mask_net.parameters(), lr=config.mask_lr)\n",
    "    model.eval()\n",
    "    data, _ = build_param_graph(model)\n",
    "    data = data.to(device)\n",
    "\n",
    "    conf_coeff = 10.0  # added confidence loss weight\n",
    "\n",
    "    for epoch in range(config.mask_epochs):\n",
    "        mask_net.train()\n",
    "        optimizer.zero_grad()\n",
    "        soft_mask = mask_net(data).view(-1)\n",
    "\n",
    "        temp_model = clone_model_with_mask(model, soft_mask)\n",
    "\n",
    "        temp_model.eval()\n",
    "        x, y = forget_data[0].to(device), torch.tensor([forget_data[1]]).to(device)\n",
    "        out = temp_model(x)\n",
    "        conf = F.softmax(out, dim=1)[0, y.item()]\n",
    "\n",
    "        loss = conf_coeff * conf + config.kl_coeff * kl_divergence(soft_mask) + config.l1_coeff * soft_mask.mean()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Optional: print grad norm\n",
    "        grad_norm = mask_net.conv2.weight.grad.norm().item() if mask_net.conv2.weight.grad is not None else 0\n",
    "        print(f\"[MaskNet] Epoch {epoch+1}/{config.mask_epochs} - Loss: {loss.item():.6f} - Confidence: {conf.item():.6f} - GradNorm: {grad_norm:.6f}\")\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    hard_mask = (soft_mask > 0.5).float().detach()\n",
    "    return hard_mask\n",
    "\n",
    "\n",
    "# Main pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    classifier = Classifier().to(config.device)\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=config.lr)\n",
    "    loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        classifier.train()\n",
    "        total_loss = 0\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(config.device), y.to(config.device)\n",
    "            optimizer.zero_grad()\n",
    "            out = classifier(x)\n",
    "            loss = F.cross_entropy(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"[Classifier] Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    print(\"Before Unlearning:\")\n",
    "    evaluate_model(classifier, test_loader, config.device)\n",
    "\n",
    "    forget_idx, forget_x, forget_y = get_forget_point()\n",
    "    forget_data = (forget_x.to(config.device), forget_y)\n",
    "\n",
    "    classifier.eval()\n",
    "    conf_before = F.softmax(classifier(forget_data[0]), dim=1)[0, forget_data[1]].item()\n",
    "    print(f\"Confidence of forget point before unlearning: {conf_before:.4f}\")\n",
    "\n",
    "    mask_net = MaskNet(1, 32, 1).to(config.device)\n",
    "    final_mask = train_mask_generator(mask_net, classifier, forget_data, config.device, config)\n",
    "    apply_mask_to_weights(classifier, final_mask)\n",
    "\n",
    "    print(\"After Unlearning:\")\n",
    "    evaluate_model(classifier, test_loader, config.device)\n",
    "\n",
    "    conf_after = F.softmax(classifier(forget_data[0]), dim=1)[0, forget_data[1]].item()\n",
    "    print(f\"Confidence of forget point after unlearning: {conf_after:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71db9459-f1d2-4434-b3ec-d36e138e2c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gc\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    batch_size: int = 128\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    lr_classifier: float = 1e-3\n",
    "    lr_mask: float = 1e-3\n",
    "    classifier_epochs: int = 10\n",
    "    mask_epochs: int = 20\n",
    "    delta: float = None  # will be set dynamically\n",
    "    kl_coeff: float = 1e-3\n",
    "    hidden_channels: int = 32\n",
    "    overlap_threshold: float = 0.1  # cosine similarity threshold\n",
    "    max_attempts: int = 50\n",
    "\n",
    "config = Config()\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_set = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_set = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=config.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=config.batch_size)\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class MaskNet(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        return torch.sigmoid(self.conv2(x, edge_index))\n",
    "\n",
    "def kl_divergence(mask_logits, prior=0.5):\n",
    "    q = torch.clamp(mask_logits, 1e-6, 1 - 1e-6)\n",
    "    return (q * torch.log(q / prior) + (1 - q) * torch.log((1 - q) / (1 - prior))).mean()\n",
    "\n",
    "def get_model_weights(model):\n",
    "    return torch.cat([param.view(-1) for param in model.parameters()])\n",
    "\n",
    "def apply_mask_to_weights(model, mask, sizes):\n",
    "    offset = 0\n",
    "    with torch.no_grad():\n",
    "        for param, size in zip(model.parameters(), sizes):\n",
    "            shape = param.shape\n",
    "            param.data *= mask[offset:offset + size].view(shape)\n",
    "            offset += size\n",
    "\n",
    "def calculate_margin(classifier, x, y):\n",
    "    classifier.eval()\n",
    "    with torch.no_grad():\n",
    "        output = classifier(x)\n",
    "        logit_y = output[0, y]\n",
    "        logit_max_others = torch.cat([output[0, :y], output[0, y+1:]]).max()\n",
    "        margin = (logit_y - logit_max_others).item()\n",
    "    return margin\n",
    "\n",
    "def is_unlearning_feasible_margin(classifier, x, y, delta):\n",
    "    margin = calculate_margin(classifier, x, y)\n",
    "    print(f\"[Feasibility Check] Margin: {margin:.4f}, Threshold: {delta}\")\n",
    "    return margin > delta\n",
    "\n",
    "def build_graph_from_model(model):\n",
    "    sizes = [param.numel() for param in model.parameters()]\n",
    "    total_params = sum(sizes)\n",
    "    x = torch.ones((total_params, 1)).to(config.device)\n",
    "    edge_index = []\n",
    "    idx = 0\n",
    "    for size in sizes:\n",
    "        for i in range(size - 1):\n",
    "            edge_index.append([idx + i, idx + i + 1])\n",
    "            edge_index.append([idx + i + 1, idx + i])\n",
    "        idx += size\n",
    "    edge_index = torch.tensor(edge_index).t().contiguous().to(config.device)\n",
    "    return x, edge_index, sizes\n",
    "\n",
    "def train_mask_generator(mask_net, classifier, forget_data, config):\n",
    "    classifier.eval()\n",
    "    x_init, edge_index, sizes = build_graph_from_model(classifier)\n",
    "    optimizer = optim.Adam(mask_net.parameters(), lr=config.lr_mask)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    x_forget, y_forget = forget_data\n",
    "\n",
    "    for epoch in range(config.mask_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        mask_logits = mask_net(x_init, edge_index).squeeze()\n",
    "        temp_model = Classifier().to(config.device)\n",
    "        temp_model.load_state_dict(classifier.state_dict())\n",
    "        apply_mask_to_weights(temp_model, mask_logits, sizes)\n",
    "        output = temp_model(x_forget)\n",
    "        ce_loss = criterion(output, y_forget.unsqueeze(0))\n",
    "        conf = F.softmax(output, dim=1)[0, y_forget].item()\n",
    "        kl = kl_divergence(mask_logits)\n",
    "        loss = ce_loss + config.kl_coeff * kl\n",
    "        loss.backward()\n",
    "        grad_norm = mask_net.conv2.lin.weight.grad.norm().item() if mask_net.conv2.lin.weight.grad is not None else 0\n",
    "        print(f\"[MaskNet] Epoch {epoch+1}/{config.mask_epochs} - Loss: {loss.item():.6f} - Confidence: {conf:.6f} - GradNorm: {grad_norm:.6f}\")\n",
    "        optimizer.step()\n",
    "        del temp_model\n",
    "        gc.collect()\n",
    "    return mask_logits.detach(), sizes\n",
    "\n",
    "def train_classifier(model, loader, config):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr_classifier)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(config.classifier_epochs):\n",
    "        total_loss = 0\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(config.device), y.to(config.device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(x), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"[Classifier] Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(config.device), y.to(config.device)\n",
    "            output = model(x).argmax(dim=1)\n",
    "            correct += (output == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return torch.dot(a, b) / (torch.norm(a) * torch.norm(b) + 1e-8)\n",
    "\n",
    "def main():\n",
    "    classifier = Classifier().to(config.device)\n",
    "    train_classifier(classifier, train_loader, config)\n",
    "\n",
    "    acc_before = evaluate(classifier, test_loader)\n",
    "    print(f\"\\nBefore Unlearning:\\nAccuracy: {acc_before:.4f}\")\n",
    "\n",
    "    # Calculate margin stats on a subset to set a realistic delta threshold\n",
    "    margins = []\n",
    "    num_samples_for_margin = 1000\n",
    "    print(\"\\nCalculating margin statistics to set delta threshold...\")\n",
    "    for i in range(num_samples_for_margin):\n",
    "        x_i, y_i = train_set[i]\n",
    "        x_i = x_i.unsqueeze(0).to(config.device)\n",
    "        y_i = torch.tensor(y_i).to(config.device)\n",
    "        margin = calculate_margin(classifier, x_i, y_i)\n",
    "        margins.append(margin)\n",
    "    margins_np = np.array(margins)\n",
    "    mean_margin = margins_np.mean()\n",
    "    std_margin = margins_np.std()\n",
    "    suggested_delta = mean_margin - std_margin\n",
    "    print(f\"Margin stats - min: {margins_np.min():.4f}, max: {margins_np.max():.4f}, mean: {mean_margin:.4f}, std: {std_margin:.4f}\")\n",
    "    print(f\"Setting feasibility margin threshold (delta) to: {suggested_delta:.4f}\")\n",
    "    config.delta = suggested_delta\n",
    "\n",
    "    accepted_masks = []\n",
    "\n",
    "    forget_x, forget_y, forget_idx, forget_mask, sizes = None, None, None, None, None\n",
    "\n",
    "    for attempt in range(1, config.max_attempts + 1):\n",
    "        idx = random.randint(0, len(train_set) - 1)\n",
    "        x_i, y_i = train_set[idx]\n",
    "        x_i = x_i.unsqueeze(0).to(config.device)\n",
    "        y_i = torch.tensor(y_i).to(config.device)\n",
    "        print(f\"\\nAttempt {attempt}: Testing forget point idx {idx} with label {y_i.item()}\")\n",
    "\n",
    "        if not is_unlearning_feasible_margin(classifier, x_i, y_i, config.delta):\n",
    "            print(\"❌ Not feasible by margin, trying another point...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"✅ Forget point idx {idx} is feasible for unlearning.\")\n",
    "\n",
    "        mask_net = MaskNet(1, config.hidden_channels, 1).to(config.device)\n",
    "        mask_logits, sizes = train_mask_generator(mask_net, classifier, (x_i, y_i), config)\n",
    "\n",
    "        overlap_ok = True\n",
    "        for prev_mask in accepted_masks:\n",
    "            sim = cosine_similarity(mask_logits, prev_mask).item()\n",
    "            if sim > config.overlap_threshold:\n",
    "                print(f\"❌ Overlap {sim:.4f} exceeds threshold with existing mask, trying next point...\")\n",
    "                overlap_ok = False\n",
    "                break\n",
    "\n",
    "        if overlap_ok:\n",
    "            print(f\"✅ Forget point idx {idx} accepted for unlearning after overlap check.\")\n",
    "            accepted_masks.append(mask_logits)\n",
    "            forget_x, forget_y, forget_idx, forget_mask = x_i, y_i, idx, mask_logits\n",
    "            break\n",
    "    else:\n",
    "        print(\"❌ No feasible forget point found within max attempts passing overlap checks.\")\n",
    "        return\n",
    "\n",
    "    output = classifier(forget_x)\n",
    "    conf_before = F.softmax(output, dim=1)[0, forget_y].item()\n",
    "    print(f\"Confidence of forget point before unlearning: {conf_before:.4f}\")\n",
    "\n",
    "    apply_mask_to_weights(classifier, forget_mask, sizes)\n",
    "\n",
    "    acc_after = evaluate(classifier, test_loader)\n",
    "    output = classifier(forget_x)\n",
    "    conf_after = F.softmax(output, dim=1)[0, forget_y].item()\n",
    "\n",
    "    print(f\"\\nAfter Unlearning:\\nAccuracy: {acc_after:.4f}\")\n",
    "    print(f\"Confidence of forget point after unlearning: {conf_after:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab6ee2-3759-4042-8589-83e632f13f84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (resnet-env)",
   "language": "python",
   "name": "resnet-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
