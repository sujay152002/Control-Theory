{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9b5ddcf-123d-4160-8c4d-90d083e32a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1: Avg Loss = 1.2058\n",
      "Epoch 2: Avg Loss = 0.3810\n",
      "Epoch 3: Avg Loss = 0.2887\n",
      "Epoch 4: Avg Loss = 0.2444\n",
      "Epoch 5: Avg Loss = 0.2222\n",
      "Epoch 6: Avg Loss = 0.1713\n",
      "Epoch 7: Avg Loss = 0.1525\n",
      "\n",
      "Evaluation before unlearning:\n",
      "Before Unlearning: Accuracy = 0.9618\n",
      "[Before Unlearning] Target sample - Pred: 5, Confidence: 0.6969, True: 5\n",
      "\n",
      "Training probabilistic mask generator (Optimal Probabilistic Masking)...\n",
      "[MaskNet] Epoch 1/7\n",
      "[MaskNet] Epoch loss: 109357472.6690\n",
      "[MaskNet] Epoch 2/7\n",
      "[MaskNet] Epoch loss: 16009.1666\n",
      "[MaskNet] Epoch 3/7\n",
      "[MaskNet] Epoch loss: 19726.5471\n",
      "[MaskNet] Epoch 4/7\n",
      "[MaskNet] Epoch loss: 20532.4908\n",
      "[MaskNet] Epoch 5/7\n",
      "[MaskNet] Epoch loss: 20743.3824\n",
      "[MaskNet] Epoch 6/7\n",
      "[MaskNet] Epoch loss: 20882.6840\n",
      "[MaskNet] Epoch 7/7\n",
      "[MaskNet] Epoch loss: 20954.3484\n",
      "k=150, confidence=0.6999\n",
      "k=175, confidence=0.5078\n",
      "k=200, confidence=0.4995\n",
      "k=225, confidence=0.5132\n",
      "k=250, confidence=0.5000\n",
      "k=275, confidence=0.5253\n",
      "k=300, confidence=0.4524\n",
      "\n",
      "Final Evaluation after unlearning:\n",
      "After Unlearning: Accuracy = 0.9590\n",
      "[After Unlearning] Target sample - Pred: 3, Confidence: 0.4534, True: 5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch_geometric.data import Data as GeoData\n",
    "from torch_geometric.nn import GCNConv\n",
    "import gc\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "@dataclass\n",
    "class MaskingConfig:\n",
    "    epochs: int = 7\n",
    "    gumbel_tau: float = 1.0      # Gumbel-Softmax temperature\n",
    "    sparsity_target: int = 150\n",
    "    max_k: int = 300\n",
    "    k_increment: int = 25\n",
    "    kl_weight: float = 0.001\n",
    "    lagrange_weight: float = 1.0 # Lagrange term for sparsity\n",
    "    learning_rate: float = 1e-3\n",
    "    batch_limit: int = 10\n",
    "    confidence_threshold: float = 0.01\n",
    "\n",
    "FEATURE_DIM = 28 * 28\n",
    "HIDDEN_DIM = 300\n",
    "HIDDEN2_DIM = 100\n",
    "OUTPUT_DIM = 10\n",
    "TARGET_LAYERS = ['fc2', 'fc3']\n",
    "\n",
    "def gumbel_sigmoid(logits, tau=1.0, hard=False, eps=1e-10):\n",
    "    \"\"\" Differentiable sampling of Bernoulli (binary mask) with Gumbel-Softmax trick. \"\"\"\n",
    "    U = torch.rand_like(logits)\n",
    "    gumbel = -torch.log(-torch.log(U + eps) + eps)\n",
    "    y = torch.sigmoid((logits + gumbel) / tau)\n",
    "    if hard:\n",
    "        y_hard = (y > 0.5).float()\n",
    "        y = (y_hard - y).detach() + y\n",
    "    return y\n",
    "\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(FEATURE_DIM, HIDDEN_DIM)\n",
    "        self.fc2 = nn.Linear(HIDDEN_DIM, HIDDEN2_DIM)\n",
    "        self.fc3 = nn.Linear(HIDDEN2_DIM, OUTPUT_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, FEATURE_DIM)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class GCNMaskGenerator(nn.Module):\n",
    "    def __init__(self, in_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 64)\n",
    "        self.conv2 = GCNConv(64, 1) # Output: predictive logit for Bernoulli mask\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        logits = self.conv2(x, edge_index).squeeze()\n",
    "        return logits # Logits for mask sampling\n",
    "\n",
    "def get_target_parameters(model: nn.Module, target_layers: List[str] = None) -> List[Tuple[str, nn.Parameter]]:\n",
    "    if target_layers is None:\n",
    "        target_layers = TARGET_LAYERS\n",
    "    return [\n",
    "        (name, param) for name, param in model.named_parameters()\n",
    "        if any(layer in name for layer in target_layers) and 'weight' in name\n",
    "    ]\n",
    "\n",
    "def construct_graph_last_layers(model: nn.Module, input_data: torch.Tensor, target: torch.Tensor) -> Tuple[GeoData, Dict[str,int]]:\n",
    "    model.zero_grad()\n",
    "    output = model(input_data)\n",
    "    loss = F.cross_entropy(output, target)\n",
    "    loss.backward()\n",
    "\n",
    "    params, grads, activ_in, activ_out, names, idx_map = [], [], [], [], [], {}\n",
    "    idx = 0\n",
    "\n",
    "    for name, param in get_target_parameters(model):\n",
    "        if param.grad is not None:\n",
    "            shape = param.shape\n",
    "            param_data = param.detach().flatten().cpu()\n",
    "            grad_data = param.grad.detach().flatten().cpu()\n",
    "            in_feat_val = shape[1] if len(shape) > 1 else 1\n",
    "            out_feat_val = shape[0]\n",
    "            in_feat = torch.full((param_data.shape[0], 1), float(in_feat_val))\n",
    "            out_feat = torch.full((param_data.shape[0], 1), float(out_feat_val))\n",
    "            for i in range(param_data.shape[0]):\n",
    "                params.append(param_data[i].unsqueeze(0))\n",
    "                grads.append(grad_data[i].unsqueeze(0))\n",
    "                activ_in.append(in_feat[i])\n",
    "                activ_out.append(out_feat[i])\n",
    "                names.append(name)\n",
    "                idx_map[f\"{name}_{i}\"] = idx\n",
    "                idx += 1\n",
    "\n",
    "    x = torch.cat([\n",
    "        torch.stack(params),\n",
    "        torch.stack(grads),\n",
    "        torch.stack(activ_in),\n",
    "        torch.stack(activ_out)\n",
    "    ], dim=1)\n",
    "\n",
    "    edge_index = []\n",
    "    n = len(names)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, min(i+10, n)):\n",
    "            if names[i].split(\".\")[0] == names[j].split(\".\")[0]:\n",
    "                edge_index.extend([[i, j], [j, i]])\n",
    "            else:\n",
    "                prefix_i = names[i].split(\".\")[0]\n",
    "                prefix_j = names[j].split(\".\")[0]\n",
    "                if (prefix_i in ['fc2', 'fc3']) and (prefix_j in ['fc2', 'fc3']):\n",
    "                    edge_index.extend([[i, j], [j, i]])\n",
    "    if len(edge_index) == 0:\n",
    "        for i in range(n-1):\n",
    "            edge_index.extend([[i, i+1], [i+1, i]])\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    model.zero_grad()\n",
    "    return GeoData(x=x, edge_index=edge_index), idx_map\n",
    "\n",
    "def compute_kl_divergence(q_probs: torch.Tensor, prior_probs: torch.Tensor) -> torch.Tensor:\n",
    "    eps = 1e-8\n",
    "    q_probs = q_probs / (q_probs.sum() + eps)\n",
    "    prior_probs = prior_probs / (prior_probs.sum() + eps)\n",
    "    q_probs = torch.clamp(q_probs, eps, 1.0)\n",
    "    prior_probs = torch.clamp(prior_probs, eps, 1.0)\n",
    "    return (q_probs * (q_probs.log() - prior_probs.log())).sum()\n",
    "\n",
    "def get_prior_distribution(model):\n",
    "    weights = []\n",
    "    for _, param in get_target_parameters(model):\n",
    "        weights.append(param.detach().flatten().abs().cpu())\n",
    "    full = torch.cat(weights)\n",
    "    return full / (full.sum() + 1e-8)\n",
    "\n",
    "def apply_mask_to_model(model, mask: torch.Tensor, idx_map: Dict[str, int]):\n",
    "    with torch.no_grad():\n",
    "        for name, param in get_target_parameters(model):\n",
    "            param_flat = param.data.view(-1)\n",
    "            mask_indices = [idx_map.get(f\"{name}_{i}\", None) for i in range(len(param_flat))]\n",
    "            mask_tensor = torch.ones_like(param_flat)\n",
    "            for i, idx in enumerate(mask_indices):\n",
    "                if idx is not None and idx < len(mask):\n",
    "                    mask_tensor[i] = 1 - mask[idx]\n",
    "            param.data.copy_((param_flat * mask_tensor).view(param.shape))\n",
    "\n",
    "def train_mask_generator(mask_net, classifier, dataset, device, prior_probs, config):\n",
    "    mask_net.train()\n",
    "    optimizer = optim.Adam(mask_net.parameters(), lr=config.learning_rate)\n",
    "    temp_model = MNISTClassifier().to(device)\n",
    "    lagrange_mult = nn.Parameter(torch.tensor([0.]), requires_grad=True)\n",
    "    lagrange_optim = optim.Adam([lagrange_mult], lr=1e-2)\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        print(f\"[MaskNet] Epoch {epoch+1}/{config.epochs}\")\n",
    "        total_loss = 0\n",
    "        for i in range(min(config.batch_limit, len(dataset))):\n",
    "            x, y = dataset[i]\n",
    "            x = x.unsqueeze(0).to(device)\n",
    "            y = torch.tensor([y], device=device)\n",
    "            graph, idx_map = construct_graph_last_layers(classifier, x, y)\n",
    "            graph = graph.to(device)\n",
    "            logits = mask_net(graph)\n",
    "            # Sample mask: stochastic, relaxed, with Gumbel noise\n",
    "            sampled_mask = gumbel_sigmoid(logits, tau=config.gumbel_tau, hard=False)\n",
    "            # Apply mask to temp copy of classifier\n",
    "            with torch.no_grad():\n",
    "                temp_model.load_state_dict(classifier.state_dict())\n",
    "            apply_mask_to_model(temp_model, sampled_mask, idx_map)\n",
    "            out = temp_model(x)\n",
    "            # Red team loss: push prediction away from target (unlearning)\n",
    "            loss_forget = F.cross_entropy(out, y)\n",
    "            # Prior regularization (KL)\n",
    "            kl = compute_kl_divergence(sampled_mask, prior_probs.to(device))\n",
    "            # Sparsity control (\"Lagrange multiplier\" penalty)\n",
    "            cardinality = sampled_mask.sum()\n",
    "            sparsity_loss = (cardinality - config.sparsity_target) ** 2\n",
    "            # Full loss\n",
    "            loss = loss_forget + config.kl_weight * kl + config.lagrange_weight * sparsity_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"[MaskNet] Epoch loss: {total_loss/config.batch_limit:.4f}\")\n",
    "\n",
    "def evaluate(model, loader, title=\"Eval\"):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(next(model.parameters()).device), target.to(next(model.parameters()).device)\n",
    "            pred = model(data).argmax(1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += target.size(0)\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    print(f\"{title}: Accuracy = {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "def main():\n",
    "    config = MaskingConfig()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    transform = transforms.ToTensor()\n",
    "    dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    target_idx = 0\n",
    "    target_x, target_y = dataset[target_idx]\n",
    "    retain_indices = [i for i in range(len(dataset)) if i != target_idx]\n",
    "    retain_subset = Subset(dataset, retain_indices[:5000])\n",
    "    retain_loader = DataLoader(retain_subset, batch_size=128, shuffle=True)\n",
    "\n",
    "    classifier = MNISTClassifier().to(device)\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "    classifier.train()\n",
    "    for epoch in range(7):\n",
    "        total_loss = 0\n",
    "        for data, target in retain_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(classifier(data), target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}: Avg Loss = {total_loss/len(retain_loader):.4f}\")\n",
    "\n",
    "    print(\"\\nEvaluation before unlearning:\")\n",
    "    evaluate(classifier, retain_loader, \"Before Unlearning\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        target_x_batch = target_x.unsqueeze(0).to(device)\n",
    "        target_y_batch = torch.tensor([target_y], device=device)\n",
    "        out = classifier(target_x_batch)\n",
    "        orig_conf = F.softmax(out, dim=1)[0, target_y].item()\n",
    "        pred = out.argmax(dim=1).item()\n",
    "        print(f\"[Before Unlearning] Target sample - Pred: {pred}, Confidence: {orig_conf:.4f}, True: {target_y}\")\n",
    "\n",
    "    prior_probs = get_prior_distribution(classifier)\n",
    "    mask_net = GCNMaskGenerator(in_channels=4).to(device)\n",
    "\n",
    "    print(\"\\nTraining probabilistic mask generator (Optimal Probabilistic Masking)...\")\n",
    "    train_mask_generator(mask_net, classifier, dataset, device, prior_probs, config)\n",
    "\n",
    "    # Final mask application: sample for this datapoint, apply progressively larger masks until confidence drops below\n",
    "    mask_net.eval()\n",
    "    data_graph, idx_map = construct_graph_last_layers(classifier, target_x_batch, target_y_batch)\n",
    "    logits = mask_net(data_graph.to(device)).detach()\n",
    "    for k in range(config.sparsity_target, config.max_k + 1, config.k_increment):\n",
    "        # Stochastic sampling, repeated 3 times to reflect distributional effect, take best\n",
    "        min_conf = 1.0\n",
    "        for rep in range(3):\n",
    "            # Relaxed sampling for evaluation; in practice, hard masking for deployment\n",
    "            mask_prob = gumbel_sigmoid(logits, tau=0.5, hard=True)\n",
    "            mask = torch.zeros_like(mask_prob)\n",
    "            topk_idxs = torch.topk(mask_prob, k).indices\n",
    "            mask[topk_idxs] = 1\n",
    "            apply_mask_to_model(classifier, mask, idx_map)\n",
    "            with torch.no_grad():\n",
    "                output = classifier(target_x_batch)\n",
    "                conf = F.softmax(output, dim=1)[0, target_y].item()\n",
    "            min_conf = min(min_conf, conf)\n",
    "        print(f\"k={k}, confidence={min_conf:.4f}\")\n",
    "        if min_conf < config.confidence_threshold:\n",
    "            break\n",
    "\n",
    "    print(\"\\nFinal Evaluation after unlearning:\")\n",
    "    evaluate(classifier, retain_loader, \"After Unlearning\")\n",
    "    with torch.no_grad():\n",
    "        out = classifier(target_x_batch)\n",
    "        pred = out.argmax(dim=1).item()\n",
    "        conf = F.softmax(out, dim=1)[0, target_y].item()\n",
    "        print(f\"[After Unlearning] Target sample - Pred: {pred}, Confidence: {conf:.4f}, True: {target_y}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a81c6-ed87-4030-b394-653ee9c9da2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (resnet-env)",
   "language": "python",
   "name": "resnet-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
