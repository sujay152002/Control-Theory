{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3379c6a1-1db8-48ac-8825-5e0a1dd8e3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training classifier...\n",
      "Epoch 1: Avg Loss = 1.2495\n",
      "Epoch 2: Avg Loss = 0.3917\n",
      "Epoch 3: Avg Loss = 0.2951\n",
      "Epoch 4: Avg Loss = 0.2439\n",
      "Epoch 5: Avg Loss = 0.2250\n",
      "Epoch 6: Avg Loss = 0.1757\n",
      "Epoch 7: Avg Loss = 0.1446\n",
      "\n",
      "Evaluating before unlearning...\n",
      "Before Unlearning: Accuracy = 0.9698\n",
      "Prior distribution size: 31000\n",
      "\n",
      "Training soft mask generator...\n",
      "[Soft Mask] Epoch 1/5\n",
      "[Soft Mask] Epoch 1 avg-loss 0.0529\n",
      "[Soft Mask] Epoch 2/5\n",
      "[Soft Mask] Epoch 2 avg-loss 0.0529\n",
      "[Soft Mask] Epoch 3/5\n",
      "[Soft Mask] Epoch 3 avg-loss 0.0529\n",
      "[Soft Mask] Epoch 4/5\n",
      "[Soft Mask] Epoch 4 avg-loss 0.0529\n",
      "[Soft Mask] Epoch 5/5\n",
      "[Soft Mask] Epoch 5 avg-loss 0.0529\n",
      "\n",
      "Training hard mask generator...\n",
      "[Mask-net] Epoch 1/5\n",
      "[Mask-net] Epoch 1 avg-loss 1.4178\n",
      "[Mask-net] Epoch 2/5\n",
      "[Mask-net] Epoch 2 avg-loss 1.4176\n",
      "[Mask-net] Epoch 3/5\n",
      "[Mask-net] Epoch 3 avg-loss 1.4178\n",
      "[Mask-net] Epoch 4/5\n",
      "[Mask-net] Epoch 4 avg-loss 1.4173\n",
      "[Mask-net] Epoch 5/5\n",
      "[Mask-net] Epoch 5 avg-loss 1.4178\n",
      "\n",
      "Applying final mask...\n",
      "Generated 31000 probability scores\n",
      "k=150, confidence=0.0039\n",
      "\n",
      "Final Results:\n",
      "After Unlearning: Accuracy = 0.8862\n",
      "Target sample - Prediction: 3, Confidence: 0.0039, Original: 5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch_geometric.data import Data as GeoData\n",
    "from torch_geometric.nn import GCNConv\n",
    "import gc\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "@dataclass\n",
    "class MaskingConfig:\n",
    "    epochs_soft: int = 5\n",
    "    epochs_hard: int = 5\n",
    "    learning_rate: float = 1e-3\n",
    "    delta: int = 150   # Increased delta\n",
    "    kl_weight: float = 0.001\n",
    "    confidence_threshold: float = 0.01\n",
    "    max_k: int = 300   # Increased max_k for iterative masking\n",
    "    k_increment: int = 25\n",
    "    max_grad_norm: float = 1.0\n",
    "    batch_limit: int = 10\n",
    "\n",
    "FEATURE_DIM = 28 * 28\n",
    "HIDDEN_DIM = 300\n",
    "HIDDEN2_DIM = 100\n",
    "OUTPUT_DIM = 10\n",
    "TARGET_LAYERS = ['fc2', 'fc3']\n",
    "\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(FEATURE_DIM, HIDDEN_DIM)\n",
    "        self.fc2 = nn.Linear(HIDDEN_DIM, HIDDEN2_DIM)\n",
    "        self.fc3 = nn.Linear(HIDDEN2_DIM, OUTPUT_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, FEATURE_DIM)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class GCNMaskGenerator(nn.Module):\n",
    "    def __init__(self, in_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 64)\n",
    "        self.conv2 = GCNConv(64, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        logits = self.conv2(x, edge_index).squeeze()\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "def get_target_parameters(model: nn.Module, target_layers: List[str] = None) -> List[Tuple[str, nn.Parameter]]:\n",
    "    if target_layers is None:\n",
    "        target_layers = TARGET_LAYERS\n",
    "    return [\n",
    "        (name, param) for name, param in model.named_parameters()\n",
    "        if any(layer in name for layer in target_layers) and 'weight' in name\n",
    "    ]\n",
    "\n",
    "def construct_graph_last_layers(model: nn.Module, input_data: torch.Tensor, target: torch.Tensor) -> Tuple[GeoData, Dict[str,int]]:\n",
    "    model.zero_grad()\n",
    "    output = model(input_data)\n",
    "    loss = F.cross_entropy(output, target)\n",
    "    loss.backward()\n",
    "\n",
    "    params, grads, activ_in, activ_out, names, idx_map = [], [], [], [], [], {}\n",
    "    idx = 0\n",
    "\n",
    "    target_params = get_target_parameters(model, TARGET_LAYERS)\n",
    "\n",
    "    # Flatten all parameters and collect their grads and activations\n",
    "    for name, param in target_params:\n",
    "        if param.grad is not None:\n",
    "            shape = param.shape\n",
    "            param_data = param.detach().flatten().cpu()\n",
    "            grad_data = param.grad.detach().flatten().cpu()\n",
    "\n",
    "            # Use in/out feature dims per weight shape: For fc layers (out_features, in_features)\n",
    "            in_feat_val = shape[1] if len(shape) > 1 else 1\n",
    "            out_feat_val = shape[0]\n",
    "\n",
    "            in_feat = torch.full((param_data.shape[0], 1), float(in_feat_val))\n",
    "            out_feat = torch.full((param_data.shape[0], 1), float(out_feat_val))\n",
    "\n",
    "            for i in range(param_data.shape[0]):\n",
    "                params.append(param_data[i].unsqueeze(0))\n",
    "                grads.append(grad_data[i].unsqueeze(0))\n",
    "                activ_in.append(in_feat[i])\n",
    "                activ_out.append(out_feat[i])\n",
    "                names.append(name)\n",
    "                idx_map[f\"{name}_{i}\"] = idx\n",
    "                idx += 1\n",
    "\n",
    "    x = torch.cat([\n",
    "        torch.stack(params), \n",
    "        torch.stack(grads), \n",
    "        torch.stack(activ_in), \n",
    "        torch.stack(activ_out)\n",
    "    ], dim=1)\n",
    "\n",
    "    # Build edges:\n",
    "    edge_index = []\n",
    "    n = len(names)\n",
    "\n",
    "    # Connect nodes from same param tensors and also across param tensors within last layers\n",
    "    # Connect nodes if they share layer prefix or in the same layer\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, min(i+10, n)):  # connect up to 10 neighbors for better info flow\n",
    "            # connect if in same param tensor (same layer name)\n",
    "            if names[i].split(\".\")[0] == names[j].split(\".\")[0]:\n",
    "                edge_index.extend([[i, j], [j, i]])\n",
    "            else:\n",
    "                # additionally connect fc2 and fc3 param nodes with edges, cross-layer edges\n",
    "                # Since both layers are connected in the model, allow cross connections between fc2.weight and fc3.weight nodes\n",
    "                prefix_i = names[i].split(\".\")[0]\n",
    "                prefix_j = names[j].split(\".\")[0]\n",
    "                if (prefix_i in ['fc2', 'fc3']) and (prefix_j in ['fc2', 'fc3']):\n",
    "                    edge_index.extend([[i, j], [j, i]])\n",
    "\n",
    "    if len(edge_index) == 0:\n",
    "        for i in range(n - 1):\n",
    "            edge_index.extend([[i, i+1], [i+1, i]])\n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    model.zero_grad()\n",
    "    return GeoData(x=x, edge_index=edge_index), idx_map\n",
    "\n",
    "def compute_kl_divergence(q_probs: torch.Tensor, prior_probs: torch.Tensor) -> torch.Tensor:\n",
    "    eps = 1e-8\n",
    "    q_probs = q_probs / (q_probs.sum() + eps)\n",
    "    prior_probs = prior_probs / (prior_probs.sum() + eps)\n",
    "    q_probs = torch.clamp(q_probs, eps, 1.0)\n",
    "    prior_probs = torch.clamp(prior_probs, eps, 1.0)\n",
    "    return (q_probs * (q_probs.log() - prior_probs.log())).sum()\n",
    "\n",
    "def get_prior_distribution(model: nn.Module) -> torch.Tensor:\n",
    "    weights = []\n",
    "    target_params = get_target_parameters(model, TARGET_LAYERS)\n",
    "    for _, param in target_params:\n",
    "        weights.append(param.detach().flatten().abs().cpu())\n",
    "    full = torch.cat(weights)\n",
    "    return full / (full.sum() + 1e-8)\n",
    "\n",
    "def apply_mask_to_model(model: nn.Module, mask: torch.Tensor, idx_map: Dict[str, int]) -> None:\n",
    "    with torch.no_grad():\n",
    "        param_idx = 0\n",
    "        target_params = get_target_parameters(model, TARGET_LAYERS)\n",
    "        for name, param in target_params:\n",
    "            param_flat = param.detach().clone().view(-1)\n",
    "            param_size = param_flat.shape[0]\n",
    "            if param_idx + param_size <= len(mask):\n",
    "                mask_slice = mask[param_idx:param_idx + param_size]\n",
    "                masked_param = param_flat * (1 - mask_slice.to(param.device))\n",
    "                param.data.copy_(masked_param.view(param.shape))\n",
    "            param_idx += param_size\n",
    "\n",
    "def safe_construct_graph(model: nn.Module, input_data: torch.Tensor, target: torch.Tensor, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return construct_graph_last_layers(model, input_data, target)\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower() and attempt < max_retries - 1:\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                continue\n",
    "            raise e\n",
    "\n",
    "def train_mask_generator(mask_net: GCNMaskGenerator, classifier: MNISTClassifier, dataset, device, prior_probs, config: MaskingConfig, soft_mask=False):\n",
    "    mask_net.train()\n",
    "    optimizer = optim.Adam(mask_net.parameters(), lr=config.learning_rate)\n",
    "    temp_model = MNISTClassifier().to(device)\n",
    "\n",
    "    epochs = config.epochs_soft if soft_mask else config.epochs_hard\n",
    "    tag = \"[Soft Mask]\" if soft_mask else \"[Mask-net]\"\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"{tag} Epoch {epoch + 1}/{epochs}\")\n",
    "        total_loss = 0\n",
    "        num_batches = min(config.batch_limit, len(dataset))\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            try:\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "                x, y = dataset[i]\n",
    "                x = x.unsqueeze(0).to(device)\n",
    "                y = torch.tensor([y], device=device)\n",
    "\n",
    "                graph, _ = safe_construct_graph(classifier, x, y)\n",
    "                graph = graph.to(device)\n",
    "\n",
    "                q_probs = mask_net(graph)\n",
    "\n",
    "                if soft_mask:\n",
    "                    # Soft mask uses q_probs directly as mask\n",
    "                    mask = q_probs\n",
    "                else:\n",
    "                    # Hard mask: top-k mask based on delta\n",
    "                    k = min(config.delta, len(q_probs))\n",
    "                    topk = torch.topk(q_probs, k).indices\n",
    "                    mask = torch.zeros_like(q_probs)\n",
    "                    mask[topk] = 1\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    temp_model.load_state_dict(classifier.state_dict())\n",
    "                apply_mask_to_model(temp_model, mask, {})\n",
    "\n",
    "                output = temp_model(x)\n",
    "                loss_pred = F.cross_entropy(output, y)\n",
    "                kl = compute_kl_divergence(q_probs, prior_probs.to(device))\n",
    "                loss = loss_pred + config.kl_weight * kl\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(mask_net.parameters(), max_norm=config.max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                del x, y, graph, q_probs, mask, output, loss_pred, kl, loss\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {i}: {e}\")\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                continue\n",
    "\n",
    "        avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "        print(f\"{tag} Epoch {epoch + 1} avg-loss {avg_loss:.4f}\")\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, title: str = \"Eval\") -> float:\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += target.size(0)\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    print(f\"{title}: Accuracy = {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "def main():\n",
    "    config = MaskingConfig()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    transform = transforms.ToTensor()\n",
    "    dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    target_idx = 0\n",
    "    target_x, target_y = dataset[target_idx]\n",
    "    retain_indices = [i for i in range(len(dataset)) if i != target_idx]\n",
    "    retain_subset = Subset(dataset, retain_indices[:5000])\n",
    "    retain_loader = DataLoader(retain_subset, batch_size=128, shuffle=True)\n",
    "\n",
    "    # Train classifier longer\n",
    "    print(\"Training classifier...\")\n",
    "    classifier = MNISTClassifier().to(device)\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "    classifier.train()\n",
    "    for epoch in range(7):  # Increased epochs\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(retain_loader):\n",
    "            optimizer.zero_grad()\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = classifier(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        avg_loss = running_loss / len(retain_loader)\n",
    "        print(f\"Epoch {epoch+1}: Avg Loss = {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"\\nEvaluating before unlearning...\")\n",
    "    evaluate(classifier, retain_loader, title=\"Before Unlearning\")\n",
    "\n",
    "    prior_probs = get_prior_distribution(classifier)\n",
    "    print(f\"Prior distribution size: {len(prior_probs)}\")\n",
    "\n",
    "    print(\"\\nTraining soft mask generator...\")\n",
    "    mask_net = GCNMaskGenerator(in_channels=4).to(device)\n",
    "    train_mask_generator(mask_net, classifier, dataset, device, prior_probs, config, soft_mask=True)\n",
    "\n",
    "    print(\"\\nTraining hard mask generator...\")\n",
    "    train_mask_generator(mask_net, classifier, dataset, device, prior_probs, config, soft_mask=False)\n",
    "\n",
    "    print(\"\\nApplying final mask...\")\n",
    "    mask_net.eval()\n",
    "    classifier.eval()\n",
    "\n",
    "    try:\n",
    "        target_x_batch = target_x.unsqueeze(0).to(device)\n",
    "        target_y_batch = torch.tensor([target_y], device=device)\n",
    "\n",
    "        classifier.train()\n",
    "        data_graph, idx_map = safe_construct_graph(classifier, target_x_batch, target_y_batch)\n",
    "        classifier.eval()\n",
    "\n",
    "        q_probs = mask_net(data_graph.to(device))\n",
    "        print(f\"Generated {len(q_probs)} probability scores\")\n",
    "\n",
    "        k = config.delta\n",
    "        while k <= config.max_k:\n",
    "            topk_idx = torch.topk(q_probs, k).indices\n",
    "            final_mask = torch.zeros_like(q_probs)\n",
    "            final_mask[topk_idx] = 1\n",
    "            apply_mask_to_model(classifier, final_mask, idx_map)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = classifier(target_x_batch)\n",
    "                conf = F.softmax(output, dim=1)[0, target_y].item()\n",
    "\n",
    "            print(f\"k={k}, confidence={conf:.4f}\")\n",
    "            if conf < config.confidence_threshold:\n",
    "                break\n",
    "            k += config.k_increment\n",
    "\n",
    "        print(\"\\nFinal Results:\")\n",
    "        evaluate(classifier, retain_loader, title=\"After Unlearning\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = classifier(target_x_batch)\n",
    "            pred = output.argmax(dim=1).item()\n",
    "            conf = F.softmax(output, dim=1)[0, target_y].item()\n",
    "        print(f\"Target sample - Prediction: {pred}, Confidence: {conf:.4f}, Original: {target_y}\")\n",
    "\n",
    "        del data_graph, q_probs, final_mask, output\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during final masking: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc80cf4e-1c77-4829-81f4-519b11bebddb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (resnet-env)",
   "language": "python",
   "name": "resnet-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
